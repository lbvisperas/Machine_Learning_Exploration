{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW3_Q3_VisperasLianne","provenance":[{"file_id":"1lWA_MZFS5wwGwk9FoTXS4Qm-WwIinpic","timestamp":1586412521703},{"file_id":"1OA3SBHEIHtylVARCND8N8hhxzDMk9_q9","timestamp":1586322181374}],"collapsed_sections":["I1hQdpGF0dAL","MNuUtSLVfZF0"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNQfNtpDKtYwqy2O25q9QqX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qvvDmny7krHu","colab_type":"text"},"source":["##Dataset properties##\n","\n","Total number of images: 82213.\n","\n","Training set size: 61488 images (one fruit or vegetable per image).\n","\n","Test set size: 20622 images (one fruit or vegetable per image).\n","\n","Multi-fruits set size: 103 images (more than one fruit (or fruit class) per image)\n","\n","Number of classes: 120 (fruits and vegetables).\n","\n","Image size: 100x100 pixels.\n","\n","Filename format: image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg) or r2_image_index_100.jpg or r3_image_index_100.jpg. \"r\" stands for rotated fruit. \"r2\" means that the fruit was rotated around the 3rd axis. \"100\" comes from image size (100x100 pixels).\n","\n","Different varieties of the same fruit (apple for instance) are stored as belonging to different classes."]},{"cell_type":"markdown","metadata":{"id":"3EAl1LXTeWgN","colab_type":"text"},"source":["## Fruits 360 with Transfer Learning ##"]},{"cell_type":"markdown","metadata":{"id":"6D-R2LaalbVC","colab_type":"text"},"source":["#### Loading the Data Set ####"]},{"cell_type":"code","metadata":{"id":"xgThiOLl9WIL","colab_type":"code","outputId":"6813725b-6e2c-4ec0-c8e4-42c4e0b819ba","executionInfo":{"status":"ok","timestamp":1586448297489,"user_tz":-420,"elapsed":76296,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}},"colab":{"base_uri":"https://localhost:8080/","height":159}},"source":["!git clone https://github.com/Horea94/Fruit-Images-Dataset.git\n","!ls\n","!cd Fruit-Images-Dataset\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'Fruit-Images-Dataset'...\n","remote: Enumerating objects: 377165, done.\u001b[K\n","remote: Total 377165 (delta 0), reused 0 (delta 0), pack-reused 377165\u001b[K\n","Receiving objects: 100% (377165/377165), 2.06 GiB | 38.14 MiB/s, done.\n","Resolving deltas: 100% (1160/1160), done.\n","Checking out files: 100% (82231/82231), done.\n","Fruit-Images-Dataset  sample_data\n","Fruit-Images-Dataset  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ZzQ0IaxlfA3","colab_type":"text"},"source":["#### Checking ImageNet Pretrained Models ####"]},{"cell_type":"code","metadata":{"id":"iyJ7RM_5lkI4","colab_type":"code","colab":{}},"source":["from numpy import mean\n","from numpy import std\n","from matplotlib import pyplot\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,BatchNormalization,Flatten,GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam, SGD, Adadelta\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import glob\n","import os\n","\n","# imports for array-handling and plotting\n","import numpy as np\n","import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","plt.rcParams.update({'figure.max_open_warning': 0})\n","\n","\n","from tensorflow.keras import applications\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k9EWT65EftQZ","colab_type":"text"},"source":["#### Image Preprocessing ####"]},{"cell_type":"code","metadata":{"id":"k-fCtQiZpiMx","colab_type":"code","colab":{}},"source":["# Setting Image Directories for Training, Validation and Testing Multiple Fruits\n","train_dir='Fruit-Images-Dataset/Training/'\n","validation_dir='Fruit-Images-Dataset/Test/'\n","testPred_dir='Fruit-Images-Dataset/test-multiple_fruits/'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOiHFuRHNI5-","colab_type":"code","colab":{}},"source":["nb_train_samples=61488\n","nb_validation_samples=20622"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbl7mf7tap9q","colab_type":"text"},"source":["## ImageNet Pre-Trained Models ##"]},{"cell_type":"code","metadata":{"id":"o-ECNxHZauNe","colab_type":"code","colab":{}},"source":["def metrics_plot(history):\n","  # plotting the metrics\n","  fig = plt.figure()\n","  plt.subplot(2,1,1)\n","  plt.plot(history.history['accuracy'])\n","  plt.plot(history.history['val_accuracy'])\n","  pyplot.title('Classification Accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='lower right')\n","\n","  plt.subplot(2,1,2)\n","  plt.plot(history.history['loss'])\n","  plt.plot(history.history['val_loss'])\n","  pyplot.title('Cross Entropy Loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['train', 'test'], loc='upper right')\n","\n","  plt.tight_layout()\n","\n","  return fig\n","\n","def eval_model(model):\n","  score = model.evaluate(X_test, Y_test, verbose=0)\n","  print('Test loss:', score[0])\n","  print('Test accuracy:', score[1]*100) \n","\n","import matplotlib.pyplot as plt\n","def compare_plot(model_hist1, model_hist2):\n","  # plotting the metrics\n","  fig = plt.figure()\n","  plt.subplot(2,1,1)\n","  # summarize history for accuracy\n","  plt.plot(model_hist1.history['val_accuracy'])\n","  plt.plot(model_hist2.history['val_accuracy'])\n","  plt.title('Classification Accuracy')\n","  plt.ylabel('accuracy')\n","  plt.xlabel('epoch')\n","  plt.legend(['Pretrained', 'W/outTransferLearning'], loc='upper left')\n","\n","  plt.subplot(2,1,2)\n","  # summarize history for loss\n","  plt.plot(model_hist1.history['val_loss'])\n","  plt.plot(model_hist2.history['val_loss'])\n","  plt.title('Cross Entropy Loss')\n","  plt.ylabel('loss')\n","  plt.xlabel('epoch')\n","  plt.legend(['Pretrained', 'W/outTransferLearning'], loc='upper left')\n","  \n","  return fig"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uulA3tQvQ-ny","colab_type":"text"},"source":["#### VGG 16 ####"]},{"cell_type":"code","metadata":{"id":"uO_V9U80NS0E","colab_type":"code","colab":{}},"source":["# importing the libraries\n","from tensorflow.keras.applications.vgg16 import VGG16\n","\n","IMAGE_SIZE = [64, 64]  # we will keep the image size as (64,64). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","vgg = VGG16(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lIQ3aOpV3FF2","colab_type":"code","outputId":"2ce71e98-f719-4dca-9322-8391434cf0cb","executionInfo":{"status":"ok","timestamp":1586453287602,"user_tz":-420,"elapsed":6737,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["from keras.applications.vgg16 import preprocess_input\n","\n","train_datagen = ImageDataGenerator(rescale=1. / 255)\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')\n","validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Found 61488 images belonging to 120 classes.\n","Found 20622 images belonging to 120 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QrIqXuq-Nn1f","colab_type":"code","outputId":"534fef48-265d-4cb1-a0ff-f75d82289fc1","executionInfo":{"status":"ok","timestamp":1586453683917,"user_tz":-420,"elapsed":1143,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}},"colab":{"base_uri":"https://localhost:8080/","height":906}},"source":["# VGG MODEL\n","# add a global spatial average pooling layer\n","for layer in vgg.layers:\n","    layer.trainable = False\n","x = vgg.output\n","x = GlobalAveragePooling2D()(x)\n","# and a fully connected output/classification layer\n","vgg_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","vgg_model = Model(inputs=vgg.input, outputs=vgg_predictions)\n","vgg_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","vgg_model.summary()"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Model: \"model_6\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 64, 64, 3)]       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","global_average_pooling2d_6 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 120)               61560     \n","=================================================================\n","Total params: 14,776,248\n","Trainable params: 61,560\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PPYtNrDR_KT2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":550},"outputId":"e0f40f1f-19f6-44e0-9033-58f7e0256d27","executionInfo":{"status":"ok","timestamp":1586455474966,"user_tz":-420,"elapsed":1777929,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["vgg_history1 = vgg_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 15,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16))  # this should be equal to total number of images in validation set."],"execution_count":28,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","3843/3843 [==============================] - 119s 31ms/step - loss: 1.4379 - accuracy: 0.7959 - val_loss: 0.8691 - val_accuracy: 0.8512\n","Epoch 2/15\n","3843/3843 [==============================] - 119s 31ms/step - loss: 0.3488 - accuracy: 0.9657 - val_loss: 0.5099 - val_accuracy: 0.8933\n","Epoch 3/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.1622 - accuracy: 0.9861 - val_loss: 0.3585 - val_accuracy: 0.9173\n","Epoch 4/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0920 - accuracy: 0.9933 - val_loss: 0.2911 - val_accuracy: 0.9270\n","Epoch 5/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0580 - accuracy: 0.9966 - val_loss: 0.2325 - val_accuracy: 0.9408\n","Epoch 6/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0393 - accuracy: 0.9977 - val_loss: 0.2115 - val_accuracy: 0.9420\n","Epoch 7/15\n","3843/3843 [==============================] - 119s 31ms/step - loss: 0.0272 - accuracy: 0.9986 - val_loss: 0.1935 - val_accuracy: 0.9472\n","Epoch 8/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0200 - accuracy: 0.9990 - val_loss: 0.1716 - val_accuracy: 0.9517\n","Epoch 9/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0152 - accuracy: 0.9993 - val_loss: 0.1585 - val_accuracy: 0.9555\n","Epoch 10/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0120 - accuracy: 0.9994 - val_loss: 0.1443 - val_accuracy: 0.9587\n","Epoch 11/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0092 - accuracy: 0.9997 - val_loss: 0.1497 - val_accuracy: 0.9568\n","Epoch 12/15\n","3843/3843 [==============================] - 119s 31ms/step - loss: 0.0071 - accuracy: 0.9998 - val_loss: 0.1458 - val_accuracy: 0.9597\n","Epoch 13/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0062 - accuracy: 0.9998 - val_loss: 0.1370 - val_accuracy: 0.9633\n","Epoch 14/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0050 - accuracy: 0.9998 - val_loss: 0.1348 - val_accuracy: 0.9630\n","Epoch 15/15\n","3843/3843 [==============================] - 118s 31ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.1373 - val_accuracy: 0.9623\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FekOqkjnaxgP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"outputId":"fd0902ba-aca2-478b-9b30-b8c2e160d16d","executionInfo":{"status":"ok","timestamp":1586455491125,"user_tz":-420,"elapsed":1744,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["metrics_plot(vgg_history1)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"jixL-6bgbgpu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":906},"outputId":"2cf3beff-877d-47ad-e71f-ab30a898c0cd","executionInfo":{"status":"ok","timestamp":1586457775839,"user_tz":-420,"elapsed":1071,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["# importing the libraries\n","from tensorflow.keras.applications import VGG16\n","\n","IMAGE_SIZE = [64, 64]  # we will keep the image size as (64,64). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","vgg2 = VGG16(input_shape = IMAGE_SIZE + [3], weights = None, include_top = False)  # input_shape = (64,64,3) as required by VGG\n","\n","# VGG MODEL\n","# add a global spatial average pooling layer\n","for layer in vgg.layers:\n","    layer.trainable = False\n","x = vgg2.output\n","x = GlobalAveragePooling2D()(x)\n","# and a fully connected output/classification layer\n","vgg2_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","vgg2_model = Model(inputs=vgg2.input, outputs=vgg2_predictions)\n","vgg2_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","vgg2_model.summary()\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Model: \"model_9\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         [(None, 64, 64, 3)]       0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n","_________________________________________________________________\n","global_average_pooling2d_9 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 120)               61560     \n","=================================================================\n","Total params: 14,776,248\n","Trainable params: 14,776,248\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9OM7Cvma-G5G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"fa3e88c4-b4ca-4ca4-f279-ca5e436f55ac"},"source":["vgg2_history = vgg2_model.fit(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 15,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16))  # this should be equal to total number of images in validation set."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","1197/3843 [========>.....................] - ETA: 18:44 - loss: 4.8157 - accuracy: 0.0150"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LkMXCANsC8m9","colab_type":"code","colab":{}},"source":["compare_plot(vgg_history,vgg2_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bcQxxcCXMmYl"},"source":["#### VGG 19 ####"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Wdcmb-ZrMmYp","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"960ae97e-4719-47a2-d15c-8a8bbc3cb6c2","executionInfo":{"status":"ok","timestamp":1586456062080,"user_tz":-420,"elapsed":4553,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["# importing the libraries\n","from tensorflow.keras.applications import VGG19\n","\n","IMAGE_SIZE = [224, 224]  # we will keep the image size as (224,224). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","vgg19 = VGG19(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (64,64,3) as required by VGG\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 3s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"prCjZfljMmYv","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"f18b639a-1edd-4505-9a2e-f49ad8a2c416","executionInfo":{"status":"ok","timestamp":1586456067912,"user_tz":-420,"elapsed":10380,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["from keras.applications.vgg19 import preprocess_input\n","\n","train_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function=preprocess_input)\n","test_datagen = ImageDataGenerator(rescale=1. / 255,preprocessing_function=preprocess_input)\n","\n","train_generator = train_datagen.flow_from_directory(train_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')\n","validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Found 61488 images belonging to 120 classes.\n","Found 20622 images belonging to 120 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xbwPZkMCMmYz","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"737b958d-42bd-417c-a1bf-4a985c69ae9d","executionInfo":{"status":"ok","timestamp":1586456067913,"user_tz":-420,"elapsed":10377,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["# VGG MODEL\n","# add a global spatial average pooling layer\n","x = vgg19.output\n","x = GlobalAveragePooling2D()(x)\n","# and a fully connected output/classification layer\n","x = Dense(128, activation = 'relu')(x)\n","vgg19_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","vgg19_model = Model(inputs=vgg19.input, outputs=vgg19_predictions)\n","vgg19_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","vgg19_model.summary()"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Model: \"model_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","global_average_pooling2d_8 ( (None, 512)               0         \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 128)               65664     \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 120)               15480     \n","=================================================================\n","Total params: 20,105,528\n","Trainable params: 20,105,528\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ox48mD6iMmY3","colab":{"base_uri":"https://localhost:8080/","height":388},"outputId":"ff8ed4f9-06ef-4a53-cdab-e8f142a837c5","executionInfo":{"status":"error","timestamp":1586457418691,"user_tz":-420,"elapsed":1361151,"user":{"displayName":"Lianne Kirsten Bonita Visperas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQ8FhQynGB8ajJUyT6ARG9ob8CuyEf2Mws6iGC=s64","userId":"10598849898920389657"}}},"source":["vgg19_history = vgg19_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 15,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16),\n","                   verbose = 1)  # this should be equal to total number of images in validation set."],"execution_count":37,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","2661/3843 [===================>..........] - ETA: 9:56 - loss: 4.9812 - accuracy: 0.0143"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-cfd9c18ca307>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                    \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                    verbose = 1)  # this should be equal to total number of images in validation set.\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m   @deprecation.deprecated(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TcfH8I_1MmY6","colab":{}},"source":["metrics_plot(vgg_history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xxDUc5xdMmY8","colab":{}},"source":["# importing the libraries\n","from tensorflow.keras.applications import VGG16\n","\n","IMAGE_SIZE = [224, 224]  # we will keep the image size as (64,64). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","vgg192 = VGG19(input_shape = IMAGE_SIZE + [3], weights = None, include_top = False)  # input_shape = (64,64,3) as required by VGG\n","\n","# VGG MODEL\n","# add a global spatial average pooling layer\n","for layer in vgg.layers:\n","    layer.trainable = False\n","x = vgg192.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(128, activation = 'relu')(x)\n","# and a fully connected output/classification layer\n","vgg192_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","vgg192_model = Model(inputs=vgg192.input, outputs=vgg192_predictions)\n","vgg192_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","vgg192_model.summary()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UX953OnBMmY9","colab":{}},"source":["vgg192_history = vgg192_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 15,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16),\n","                   verbose = 1)  # this should be equal to total number of images in validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A7jkGQtwMmZB","colab":{}},"source":["compare_plot(vgg_history,vgg2_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RmwI_jAznvZX","colab_type":"text"},"source":["#### XCEPTION ####"]},{"cell_type":"code","metadata":{"id":"VkH0vGsnnyUS","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.xception import Xception, preprocess_input\n","\n","IMAGE_SIZE = [299, 299]  # we will keep the image size as (299,299). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","xception_base = Xception(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (224,224,3) as required by VGG\n","#xception_base = Xception(weights = 'imagenet', include_top = False)  # input_shape = (299,299,3) as required by VGG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYiloL1iuocA","colab_type":"code","colab":{}},"source":["train_generator = train_datagen.flow_from_directory(train_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')\n","validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=IMAGE_SIZE,batch_size=16,class_mode='categorical')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"arVDVeoiFMR7","colab_type":"code","colab":{}},"source":["# add a global spatial average pooling layer\n","x = xception_base.output\n","x = GlobalAveragePooling2D()(x)\n","#add a fully-connected layer\n","x=Dense(512, activation = 'relu')(x)\n","# and a fully connected output/classification layer\n","xception_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","xception_model = Model(inputs=xception_base.input, outputs=xception_predictions)\n","xception_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","xception_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2hfxfgwqrL8J","colab_type":"code","colab":{}},"source":["xception_history = xception_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 10,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16),\n","                   shuffle = True, verbose = 1)  # this should be equal to total number of images in validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aN6bxcTl3zOO","colab_type":"code","colab":{}},"source":["metrics_plot(xception_history)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ILuo_zeF33hX","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.xception import Xception, preprocess_input\n","\n","IMAGE_SIZE = [299, 299]  # we will keep the image size as (299,299). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","xception_base2 = Xception(input_shape = IMAGE_SIZE + [3], weights = None, include_top = False)  # input_shape = (224,224,3) as required by VGG\n","#xception_base = Xception(weights = 'imagenet', include_top = False)  # input_shape = (224,224,3) as required by VGG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kNM2GDuj_EXt","colab_type":"code","colab":{}},"source":["# add a global spatial average pooling layer\n","x = xception_base2.output\n","x = GlobalAveragePooling2D()(x)\n","#add a fully-connected layer\n","x=Dense(512, activation = 'relu')(x)\n","# and a fully connected output/classification layer\n","xception2_predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","xception2_model = Model(inputs=xception_base2.input, outputs=xception2_predictions)\n","xception2_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","xception2_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1h-WSgGY_R9x","colab_type":"code","colab":{}},"source":["xception2_history = xception2_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 10,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16),\n","                   shuffle = True, verbose = 1)  # this should be equal to total number of images in validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T4zr1Yt_F84K","colab_type":"code","colab":{}},"source":["compare_plot(xception_history,xception2_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SUlYoCHq5eK-"},"source":["#### ResNet50 ####"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7qHHGDvV5eLF","colab":{}},"source":["from tensorflow.keras.applications.resnet import ResNet50\n","\n","IMAGE_SIZE = [224, 224]  # we will keep the image size as (299,299). You can increase the size for better results. \n","\n","# loading the weights of VGG16 without the top layer. These weights are trained on Imagenet dataset.\n","inception_base = Xception(input_shape = IMAGE_SIZE + [3], weights = 'imagenet', include_top = False)  # input_shape = (224,224,3) as required by VGG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_TknJgtE5eLS","colab":{}},"source":["train_generator = train_datagen.flow_from_directory(train_dir,target_size=IMAGE_SIZE,batch_size=batch_size,class_mode='categorical')\n","validation_generator = test_datagen.flow_from_directory(validation_dir,target_size=IMAGE_SIZE,batch_size=batch_size,class_mode='categorical')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Kv5jHl8i5eLb","colab":{}},"source":["# add a global spatial average pooling layer\n","x = inception_base.output\n","x = GlobalAveragePooling2D()(x)\n","# add a fully-connected layer\n","x = Dense(512, activation='relu')(x)\n","# and a fully connected output/classification layer\n","predictions = Dense(120, activation='softmax')(x)\n","# create the full network so we can train on it\n","resnet50_model = Model(inputs=inception_base.input, outputs=predictions)\n","resnet50_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","resnet50_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"O_hEUt2C5eLk","colab":{}},"source":["restnet50_history = inception_model.fit_generator(train_generator,\n","                   steps_per_epoch = int(nb_train_samples / 16),  # this should be equal to total number of images in training set. But to speed up the execution, I am only using 10000 images. Change this for better results. \n","                   epochs = 10,  # change this for better results\n","                   validation_data = validation_generator,\n","                   validation_steps = int(nb_validation_samples / 16),\n","                   shuffle = True, verbose = 1)  # this should be equal to total number of images in validation set."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5_EXvCj_5eLt","colab":{}},"source":["metrics_plot(resnet50_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-2yQN2ho5eL3"},"source":[""]},{"cell_type":"code","metadata":{"id":"PbC47a-BxsfY","colab_type":"code","colab":{}},"source":["import tensorflow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"43kwJgpZopSb","colab_type":"code","colab":{}},"source":["from numpy import mean\n","from numpy import std\n","from matplotlib import pyplot\n","from sklearn.model_selection import KFold\n","from tensorflow.keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dropout,Dense,BatchNormalization,Flatten,GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam, SGD, Adadelta\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import cv2\n","import glob\n","import os\n","\n","# imports for array-handling and plotting\n","import numpy as np\n","import matplotlib\n","matplotlib.use('agg')\n","import matplotlib.pyplot as plt\n","plt.rcParams.update({'figure.max_open_warning': 0})\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k3-LaNumg7hM","colab_type":"text"},"source":["We ensure that the training_fruit_img and training_label_id shapes are at par with each other."]},{"cell_type":"code","metadata":{"id":"2QmbdKPFmFjA","colab_type":"code","colab":{}},"source":["training_fruit_img.shape,training_label_id.shape"],"execution_count":0,"outputs":[]}]}